#+TITLE: Introduction to TensorFlow
#+SUBTITLE: Hackerschool, NUS Hackers
#+DATE: 2017/10/14
#+AUTHOR: Raynold Ng
#+EMAIL: raynold.ng24@gmail.com
#+OPTIONS: ':nil *:t -:t ::t <:t H:3 \n:nil ^:t arch:headline
#+OPTIONS: author:t c:nil creator:comment d:(not "LOGBOOK") date:t
#+OPTIONS: e:t email:nil f:t inline:t num:nil p:nil pri:nil stat:t
#+OPTIONS: tags:t tasks:t tex:t timestamp:t toc:nil todo:t |:t
#+DESCRIPTION:
#+EXCLUDE_TAGS: noexport
#+KEYWORDS:
#+LANGUAGE: en
#+SELECT_TAGS: export

#+WWW: http://www.nushackers.org/
#+TWITTER: @nushackers

#+FAVICON: images/tensorflow.png
#+ICON: images/tensorflow.png
#+HASHTAG: #hackerschool #tensorflow
* Load I/O Slides                                                  :noexport:
#+BEGIN_SRC emacs-lisp :tangle no
  (require 'ox-ioslide)
#+END_SRC
* Agenda
  :PROPERTIES:
  :END:
- Installation and Setup
- Machine Learning Primer
- What's TensorFlow?
- Basic TensorFlow concepts
- MNIST Example
  - Softmax
  - Convoluted Neural Networks
* NUS Hackers

- student-run organization committed to the spread of hacker culture & free/open-source software
- Events:
  - [[https://www.nushackers.org/fridayhacks/][Friday Hacks]]
  - [[http://school.nushackers.org/][hackerschool]]
  - [[https://www.facebook.com/groups/nushackers/permalink/1186836361416932/][Project Intern]]

* Installation and Setup

Ensure that you have the following installed:
1. TensorFlow: https://www.tensorflow.org/install/
2. Python 3: https://www.python.org/downloads/
3. Jupyter (recommended): http://jupyter.org/

Materials are available [[https://github.com/raynoldng/hackerschool-tensorflow][here]]
* Machine Learning Primer
  :PROPERTIES:
  :SLIDE:    segue dark quote
  :ASIDE:    right bottom
  :ARTICLE:  flexbox vleft auto-fadein
  :END:
** What is Machine Learning?
- "The science of getting computers to act *without being explicitly programmed*" - Andrew Ng
- Primary aim is to allow computers to learn automatically without human
  intervention or assistance and adjust actions accordingly


#+BEGIN_CENTER
#+ATTR_HTML: :width 40%
[[file:images/machine_learning.png]]
#+END_CENTER
** Using TensorFlow to sort cucumbers
- [[https://cloud.google.com/blog/big-data/2016/08/how-a-japanese-cucumber-farmer-is-using-deep-learning-and-tensorflow][Makoto Koike]] used deeping learning and TensorFlow to sort cucumbers by size, shape, color and other attributes

#+BEGIN_CENTER
#+ATTR_HTML: :width 100%
[[file:images/cucumber.png]]
#+END_CENTER

** Structure in data
- some interpretations to "structure in data"
  - given some data, one can predict other data points with some confidence
  - one can compress the data, i.e., store the same amount of information, with
    less space

\begin{align*}
A = \{1, 2, 6, 4, 7, 9, 0\} \\
B = \{1, 2, 1, 2, 1, 2, 1\}
\end{align*}

- we might say that $A$ has apparent structure while $B$ does not

*** Entropy
- quantified as Entropy of Process
$$H(X) = -\sum_{i=1}^{N} p(x_i) \log p(x_i)$$
- If entropy increases, uncertainty in prediction increases
*** Entropy (examples)
- Example: fair dice
$$H(\text{fair dice roll}) = -\sum_{i=1}^6 \frac{1}{6} \log \frac{1}{6}=2.58$$
- Example: biased 20:80 coin
$$H(20/80 \text{ coin toss}) = -\frac{1}{5}\log \frac{1}{5}-\frac{4}{5}\log \frac{4}{5} = 0.72$$
- biased coin toss has lower entropy; predicting its outcome is easier than a fair dice
** What are Tensors?
Recall from linear algebra that:
- Scalar: an array in 0-D
- Vector: an array in 1-D
- Matrix: an array in 2-D

All are *tensors* of n-order. Similary, tensors can be transformed with
operations. TensorFlow provides library of algorithms to perform tensor operations efficiently. 
** Example
Simple linear regression model:

$$w_o + w_1 x = \hat{y}$$

- $w_0$ and $w_1$ are *weights*, that are determined during training
- $\hat{y}$ is the predicted outcome, to be compared with actual observations $y$
- Goal: build a model that can find values of $w_0$ and $w_1$ that minimize prediction error
** Graph Representation of ML Models

Can represent linear regression as a graph
#+ATTR_HTML: :width 40%
[[file:images/linear_reg_graph.png]]

- operations are represented as nodes
- graph shows how data is transformed by nodes and what is passed between them
** Graph Representation of ML Models (1)
#+ATTR_HTML: :width 50%
[[file:images/neural_net.png]]

$$a_i^{(2)} = g(w_{i0} + w_{i1}x_1 + w_{i2}x_2 + w_{i3}x_3)$$

For more complex models, it could be helpful to visualize your graph.
[[https://www.tensorflow.org/versions/r0.7/how_tos/graph_viz/index.html][TensorBoard]] provides this virtualization tool
** Activation Functions
- A popular function is the rectified linear unit (*ReLU*):
$$g(u) = max(0, u)$$

#+BEGIN_CENTER
#+ATTR_HTML: :width 70%
[[file:images/relu.png]]
#+END_CENTER

** Gradient Descent

- a way to minimize objective function
- one takes steps proportional to the negative of the gradient of the function at the current point.

#+ATTR_HTML: :width 50%
[[file:images/gradient_descent.png]]
** Model Output
- output depends on activation function used, but is generally any real number $[-\infty, \infty]$
- For binary classification, an additional sigmoid function can be applied to
  bring the output to range of $[0,1]$
$$S(x) = \frac{1}{1+e^{-x}}$$

#+BEGIN_CENTER
#+ATTR_HTML: :width 90%
[[file:images/sigmoid.png]]
#+END_CENTER
** Softmax Function
- for multi-class prediction a softmax function is used:
$$S_j(\boldsymbol{z}) = \frac{e^{z_j}}{\sum_{k=1}^K e^{z_k}} \text{ for }j=1,\dots,k$$
- squash $K$ dimensional vector *z* to a $K$ dimensional vector that sum to 1
$$\sum_{j=1}^k S_j(\boldsymbol{z}) = 1$$
- state usually represented with *one-hot encoding*, e.g for dice roll 3: $(0,0,1,0,0,0)$
* Basic TensorFlow Concepts
  :PROPERTIES:
  :SLIDE:    segue dark quote
  :ASIDE:    right bottom
  :ARTICLE:  flexbox vleft auto-fadein
  :END:
** What is TensorFlow?
- "TensorFlow is an interface for expressing machine learning algorithms, and an implementation for executing such algorithms"
- Originally developed Google Brain Team to conduct machine learning research and deep neural networks research
- General enough to be applicable to a wide variety of other domains
** Data Flow Graphs
Tensorflow separates definition of computations from their execution

Phases:
1. assemble the graph
2. use a =session= to execute operations in the graph

#+BEGIN_SRC python
import tensorflow as tf
a = tf.add(3,5)
#+END_SRC

** How to get value of =a=?
#+BEGIN_SRC python
print(a)
#+END_SRC

Create a =session=, and within it, evaluate the graph

#+BEGIN_SRC python
sess = tf.Session()
print(sess.run(a))
sess.close()
#+END_SRC

Alternatively:

#+BEGIN_SRC python
with tf.Session() as sess:
    print(sess.run(a))
#+END_SRC

** Visualizing with TensorBoard
:PROPERTIES:
:ARTICLE:  smaller
:END:
- =tf.summary.FileWriter= serializes the graph into a format the TensorBoard can read

#+BEGIN_SRC python
tf.summary.FileWriter("logs", tf.get_default_graph()).close()
#+END_SRC

- in the same directory, run:

#+BEGIN_SRC sh
tensorboard --logdir=logs
#+END_SRC

Or in Jupyter:
#+BEGIN_SRC python
!tensorboard --logdir=logs
#+END_SRC

- This will launch an instance of TensorBoard that you can access at http://localhost:6006

** Practice with More Graphs

Try to generate the following graph: $(x+y)^{xy}$ where $x=2,y=3$

#+BEGIN_CENTER
#+ATTR_HTML: :width 70%
[[file:images/graph2.png]]
#+END_CENTER

Useful functions: =tf.add=, =tf.multiply=, =tf.pow=

** Solution

#+BEGIN_SRC python
x = 2
y = 3
op1 = tf.add(x, y)
op2 = tf.multiply(x, y)
op3 = tf.pow(op1, op2)
with tf.Session() as sess:
    op3 = sess.run(op3)
#+END_SRC

** TensorFlow Variables

- TensorFlow variables used to represent shared, persistant state manipulated by your program
- Variables hold and update parameters in your model during training
- Variables contain tensors
- Variables must be initialized unless it is a constant

#+BEGIN_SRC python
  W1 = tf.ones((2,2))
  W2 = tf.Variable(tf.zeros((2,2)), name="weights")

  with tf.Session() as sess:
      print(sess.run(W1))
      sess.run(tf.global_variables_initializer())
      print(sess.run(W2))

#+END_SRC

** Creating Variables

To create a 3-dimensional variable with shape =[1,2,3]=:

#+BEGIN_SRC python
    my_var = tf.get_variable("my_var", [1,2,3])
#+END_SRC

You may optionally specify the =dtype= and initializer to =tf.get_variable=:

#+BEGIN_SRC python
  my_int_variable = tf.get_variable("my_int_variable", [1, 2, 3],
                                    dtype=tf.int32,
                                    initializer=tf.zeros_initializer)
#+END_SRC

Can initialize a =tf.Variable= to have the value of a =tf.Tensor=:

#+BEGIN_SRC python
  other_variable = tf.get_variable("other_variable", dtype=tf.int32, 
    initializer=tf.constant([23, 42]))
#+END_SRC

** Updating Variable State

Use =tf.assign= to assign a value to a variable

#+BEGIN_SRC python
state = tf.Variable(0, name="counter")
new_value = tf.add(state, tf.constant(1))
update = tf.assign(state, new_value)

with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    print(sess.run(state))
    for _ in range(3):
        sess.run(update)
        print(sess.run(state))
#+END_SRC

** Fetching Variable State

#+BEGIN_SRC python
input1 = tf.constant(3.0)
input2 = tf.constant(2.0)
input3 = tf.constant(5.0)
intermed = tf.add(input2, input3)
mul = tf.multiply(input1, intermed)

with tf.Session() as sess:
    result = sess.run([mul, intermed])
    print(result)
#+END_SRC

** TensorFlow Placeholders

- =tf.placeholder= variables represent our input data
- =feed_dict= is a python dictionary that maps =tf.placeholder= variables to data

#+BEGIN_SRC python
input1 = tf.placeholder(tf.float32)
input2 = tf.placeholder(tf.float32)

output = tf.multiply(input1, input2)

with tf.Session() as sess:
    print(sess.run([output], feed_dict={input1:[7.], input2:[2.]}))
#+END_SRC

** Example: Linear Regression
  :PROPERTIES:
  :SLIDE:    segue dark quote
  :ASIDE:    right bottom
  :ARTICLE:  flexbox vleft auto-fadein
  :END:
*** Imports
#+BEGIN_SRC python
  import tensorflow as tf
  import numpy as np
  import seaborn
  import matplotlib.pyplot as plt
  # %matplotlib inline

#+END_SRC
*** Recap
- we have two weights $w_0$ and $w_1$, we want the model to figure out good weights by minimizing prediction error
- define the following *loss function*

$$L = \sum (y - \hat{y})^2$$

Given the following function, fit a linear model

$$y = x + 20 \sin(x/10)$$
*** Plot Input Data                                                :noexport:
Make sure that =seaborn= and =matplotlib= are installed. If you are using Jupyter, add =%matplotlib inline= in the code cell.

#+BEGIN_SRC python
import tensorflow as tf
import numpy as np
import seaborn
import matplotlib.pyplot as plt
%matplotlib inline
# Define input data
X_data = np.arange(100, step=.1)
y_data = X_data + 20 * np.sin(X_data/10)
# Plot input data
plt.scatter(X_data, y_data)
#+END_SRC
*** Scatter Plot
#+BEGIN_CENTER
#+ATTR_HTML: :width 130%
[[file:images/sample_data.png]]
#+END_CENTER
*** Define Variables and Placeholders
#+BEGIN_SRC python
# Define data size and batch size
n_samples = 1000
batch_size = 100

# TensorFlow is particular about shapes, so resize
X_data = np.reshape(X_data, (n_samples, 1))
y_data = np.reshape(y_data, (n_samples, 1))

# Define placeholders for input
X = tf.placeholder(tf.float32, shape=(batch_size, 1))
y = tf.placeholder(tf.float32, shape=(batch_size, 1))
#+END_SRC
*** Loss Function
Loss function is defined as:
$$J(W,b) = \frac{1}{N}\sum_{i=1}^{N}(y_i-(W_{x_i}+b))^2$$

#+BEGIN_SRC python
  # Define variables to be learned
  W = tf.get_variable("weights", (1,1),
                      initializer = tf.random_normal_initializer())
  b = tf.get_variable("bias", (1,),
                      initializer = tf.constant_initializer(0.0))
  y_pred = tf.matmul(X, W) + b
  loss = tf.reduce_sum((y - y_pred)**2/n_samples)
#+END_SRC
*** Define Optimizer and Train Model
:PROPERTIES:
:ARTICLE:  smaller
:END:
#+BEGIN_SRC python
  # Define optimizer operation
  opt_operation = tf.train.AdamOptimizer().minimize(loss)
  with tf.Session() as sess:
      # Initialize all variables in graph
      sess.run(tf.global_variables_initializer())
      # Gradient descent for 500 steps:
      for _ in range(500):
          # Select from random mini batch
          indices = np.random.choice(n_samples, batch_size)
          X_batch, y_batch = X_data[indices], y_data[indices]
          # Do gradient descent step
          _, loss_val = sess.run([opt_operation, loss],
                                 feed_dict={X: X_batch, y: y_batch})
      print(sess.run([W, b]))
      # Display results
      plt.scatter(X_data, y_data)
      plt.scatter(X_data, sess.run(W) * X_data + sess.run(b), c='g')

#+END_SRC
*** Results

#+BEGIN_CENTER
#+ATTR_HTML: :width 130%
[[file:images/trained_model.png]]
#+END_CENTER

* MNIST and TensorFlow
  :PROPeRTIES:
  :SLIDE:    segue dark quote
  :ASIDE:    right bottom
  :ARTICLE:  flexbox vleft auto-fadein
  :END:
** Introduction
- MNIST is the hello world of machine learning
- Simple computer vision dataset, consists of images of handwritten digits
- We are going to train a model to predict what the digits are

#+BEGIN_CENTER
#+ATTR_HTML: :width 80%
[[file:images/MNIST.png]]
#+END_CENTER
*** Importing MNIST Data

To download and read in the data automatically:

#+BEGIN_SRC python
from tensorflow.examples.tutorials.mnist import input_data
mnist = input_data.read_data_sets("MNIST_data/", one_hot=True)
#+END_SRC

One hot encoding
- labels have been converted to a vector of length equal to number of classes. 
- the ith element is 1, rest are 0. E.g. Digit 1: $[0,1,\dots]$
*** MNIST Data
The MNIST data is split into three parts:
1. 55,000 data points of training data (=mnist.train=)
2. 10,000 data points of test data (=mnist.test=)
3. 5,000 data points of validation data (=mnist.validation=)

Every MNIST data has 2 parts:
1. an image of a handwritten digit (call it "x")
2. corresponding label (call it "y")
** Softmax Regression
  :PROPERTIES:
  :SLIDE:    segue dark quote
  :ASIDE:    right bottom
  :ARTICLE:  flexbox vleft auto-fadein
  :END:
*** Overview
#+ATTR_HTML: :width 80%
[[file:images/softmax_1.png]]
*** Overview (1)
#+BEGIN_CENTER
#+ATTR_HTML: :width 120%
[[file:images/softmax_2.png]]
#+END_CENTER

#+BEGIN_CENTER
#+ATTR_HTML: :width 120%
[[file:images/softmax_3.png]]
#+END_CENTER
*** Data Dimensions

#+BEGIN_SRC python
  img_size = 28
  img_size_flat = img_size * img_size
  img_shape = (img_size, img_size)
  num_classes = 10
#+END_SRC

#+BEGIN_CENTER
#+ATTR_HTML: :width 100%
[[file:images/mnist_7.png]]
#+END_CENTER

*** Defining Our Model
#+BEGIN_SRC python
  x = tf.placeholder(tf.float32, [None, img_size_flat])
  y_true = tf.placeholder(tf.float32, [None, num_classes])
  y_true_cls = tf.placeholder(tf.int64, [None])
#+END_SRC

- =x= is a =placeholder=, value that we will input when we ask TensorFlow to run
- represent MNIST image as a 2-D tensor of floating numbers of shape =[None, 784]=
- =None= means that =x= can be of any length
*** Variables to be Optimized

#+BEGIN_SRC python
  weights = tf.Variable(tf.zeros([img_size_flat, num_classes]))
  biases = tf.Variable(tf.zeros([num_classes]))
#+END_SRC
- weights has a shape of =[784,10]= as we want to 784-dimensional image vectors
  by =weights= to produce 10-dimensional vectors of evidence
- biases has a shape of [10] as we can add it to the output.
*** Model

- multiples the images in the placeholder variable =x= with =weight= and =biases=
- Result is a matrix of shape =[num_images, 10]= and =W= has shape =[784, 10]=.
- =logits= is typical TensorFlow terminology
#+BEGIN_SRC python
  logits = tf.matmul(x, weights) + biases
  y_pred = tf.nn.softmax(logits)
  y_pred_cls = tf.argmax(y_pred, axis = 1)
#+END_SRC
*** Optimization Method
#+BEGIN_SRC python
  cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits,
                                                          labels=y_true)
  cost = tf.reduce_mean(cross_entropy)
  optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.5).minimize(cost)
  correct_prediction = tf.equal(y_pred_cls, y_true_cls)
  accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
#+END_SRC

*** TensorFlow Run
#+BEGIN_SRC python
  def optimize(num_iterations):
      for i in range(num_iterations):
          x_batch, y_true_batch = mnist.train.next_batch(batch_size)
          feed_dict_train = {x: x_batch,
                             y_true: y_true_batch}
          session.run(optimizer, feed_dict=feed_dict_train)
#+END_SRC

Using small batches of random data is called *stochastic training*, it is more
feasible than training on the entire data set

*** Evaluating Our Model

#+BEGIN_SRC python
  feed_dict_test = {x: mnist.test.images,
                    y_true: mnist.test.labels,
                    y_true_cls: mnist.test.cls}

  def print_accuracy():
      # Use TensorFlow to compute the accuracy.
      acc = session.run(accuracy, feed_dict=feed_dict_test)
    
      # Print the accuracy.
      print("Accuracy on test-set: {0:.1%}".format(acc))
#+END_SRC

Approx 91% is very bad, 6 digit ZIP code would have an accuracy rate of 57% 
** Convolutional Neural Network
  :PROPERTIES:
  :SLIDE:    segue dark quote
  :ASIDE:    right bottom
  :ARTICLE:  flexbox vleft auto-fadein
  :END:
*** Flowchart
#+BEGIN_CENTER
#+ATTR_HTML: :width 100%
[[file:images/cnn_network_flowchart.png]]
#+END_CENTER
*** Introduction
- Convolutional Networks work by moving smaller filter across the input image
- Filters are re-used for recognizing patters throughout the entire input image
- This makes Convolutional Networks much more powerful than Fully-Connected
  networks with the same number of variables
*** Features
#+BEGIN_CENTER
#+ATTR_HTML: :width 100%
[[file:images/features.png]]
#+END_CENTER
*** Features (1)
#+BEGIN_CENTER
#+ATTR_HTML: :width 60%
[[file:images/features_2.png]]
#+END_CENTER
*** Convolution
#+BEGIN_CENTER
#+ATTR_HTML: :width 80%
[[file:images/convolution.png]]
#+END_CENTER
*** Convolution (1)
#+BEGIN_CENTER
#+ATTR_HTML: :width 100%
[[file:images/convolution_2.png]]
#+END_CENTER
*** Convolution (2)
#+BEGIN_CENTER
#+ATTR_HTML: :width 95%
[[file:images/convolution_3.png]]
#+END_CENTER
*** Pooling
#+BEGIN_CENTER
#+ATTR_HTML: :width 80%
[[file:images/pooling.png]]
#+END_CENTER
*** Pooling (1)
#+BEGIN_CENTER
#+ATTR_HTML: :width 80%
[[file:images/pooling_2.png]]
#+END_CENTER
*** Fully Connected Layers
#+BEGIN_CENTER
#+ATTR_HTML: :width 100%
[[file:images/layers.png]]
#+END_CENTER
*** Hyper Parameters
- Convolution:
  - Number of features
  - Size of features
- Pooling
  - Window size
  - Window stride
- Fully Connected
  - number of neurons
*** Weight Initialization
Helper functions to create ReLU neurons

#+BEGIN_SRC python
  def weight_variable(shape):
    initial = tf.truncated_normal(shape, stddev=0.05)
    return tf.Variable(initial)

  def new_biases(length):
      return tf.Variable(tf.constant(0.05, shape=[length]))
#+END_SRC
*** Creating a new Convolutional Layer

Input is a 4-dim tensor:
1. image number
2. y-axis of each image
3. x-axis of each image
4. channels of each image

Output is another 4-dim tensor:
1. image number, same as input
2. y-axis of each image, might be smaller if pooling is used
3. x-axis of each image, might be smaller if pooling is used
4. channels produced by the convolutional filters
*** Helper Function for Creating a New Layer

#+BEGIN_SRC python

  def new_conv_layer(input,              # The previous layer.
                     num_input_channels, # Num. channels in prev. layer.
                     filter_size,        # Width and height of each filter.
                     num_filters,        # Number of filters.
                     use_pooling=True):  # Use 2x2 max-pooling.
      # ...

      return layer, weights
#+END_SRC
*** Flattening a Layer

- convolutional layer produces an output tensor with 4 dimensions
- fully connected layer will reduce 4-dim tensor to a 2-dim tensor that can be used as input to the fully connected layer

#+BEGIN_SRC python
  def flatten_layer(layer):
      # ...

      # return both the flatten layer and number of features
      return layer_flat, num_features

#+END_SRC
*** Creating a Fully-Connected Layer

Assumed that input is a 2-dim tensor of shape =[num_images, num_inputs]=, output is a 2-dim tensor of shape =[num_images, num_outputs]=

#+BEGIN_SRC python
  def new_fc_layer(input,          # The previous layer.
                   num_inputs,     # Num. inputs from prev. layer.
                   num_outputs,    # Num. outputs.
                   use_relu=True): # Use Rectified Linear Unit (ReLU)?
      # create new weights and biases
      # calculate new layer
      # use ReLU?

      return layer
#+END_SRC
*** Placeholder Variables

- =x= is the placeholder variable for input images
  - data-type is set to =float32=
  - shape is set to =[None, img_size_flat]=
- convolutional layers expect =x= to be encoded as a 4-dim tensor, so its shape
  is =[num_images, img_height, img_width, num_channels]=
- also have placeholder for true labels

#+BEGIN_SRC python
  x = tf.placeholder(tf.float32, shape=[None, img_size_flat], name='x')
  x_image = tf.reshape(x, [-1, img_size, img_size, num_channels])
  y_true = tf.placeholder(tf.float32, shape=[None, num_classes], name='y_true')
  y_true_cls = tf.argmax(y_true, axis=1)
#+END_SRC

*** First Convolutional Layer

- takes =x_image= as input and creates =num_filters1= different filters
  - each filter has width and height equal to filter_size1=
- down sample the image so its half the size by using max-pooling
#+BEGIN_SRC python
  layer_conv1, weights_conv1 = \
      new_conv_layer(input=x_image,
                     num_input_channels=num_channels,
                     filter_size=filter_size1,
                     num_filters=num_filters1,
                     use_pooling=True)
#+END_SRC
*** Second Convolutional Layer
- takes as input the output from the first convolutional layer
- number of iunput channels = number of filters in the first convolutional layer

#+BEGIN_SRC python
  layer_conv2, weights_conv2 = \
      new_conv_layer(input=layer_conv1,
                     num_input_channels=num_filters1,
                     filter_size=filter_size2,
                     num_filters=num_filters2,
                     use_pooling=True)
#+END_SRC
*** Flatten Layer

- use output of convolutional layer as input to a fully-connected network, which
  requires for the tensors to be reshaped to a 2-dim tensors

#+BEGIN_SRC python
  layer_flat, num_features = flatten_layer(layer_conv2)
#+END_SRC

*** Fully-Connected Layer 1

#+BEGIN_SRC python
  layer_fc1 = new_fc_layer(input=layer_flat,
                           num_inputs=num_features,
                           num_outputs=fc_size,
                           use_relu=True)
#+END_SRC
*** Fully-Connected Layer 2

#+BEGIN_SRC python
  layer_fc2 = new_fc_layer(input=layer_fc1,
                           num_inputs=fc_size,
                           num_outputs=num_classes,
                           use_relu=False)
#+END_SRC
*** Cost Function and Optimization Method

#+BEGIN_SRC python
  y_pred = tf.nn.softmax(layer_fc2)
  y_pred_cls = tf.argmax(y_pred, axis=1)
  cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=layer_fc2,
                                                          labels=y_true)
  cost = tf.reduce_mean(cross_entropy)
  optimizer = tf.train.AdamOptimizer(learning_rate=1e-4).minimize(cost)
  correct_prediction = tf.equal(y_pred_cls, y_true_cls)
  accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
#+END_SRC

** Saving and Restoring your model
  :PROPERTIES:
  :SLIDE:    segue dark quote
  :ASIDE:    right bottom
  :ARTICLE:  flexbox vleft auto-fadein
  :END:

*** Exporting the Model
- We can export the model for use in our own applications
- use =tf.train.Saver= to save the graph and the trained weights
#+BEGIN_SRC python
model_path = "./tmp/model.ckpt"
save_path = saver.save(sess, model_path) # saver is not declared???
print("Model saved in file: %s" % save_path)
#+END_SRC

*** Restoring the Session
#+BEGIN_SRC python
saver = tf.train.Saver()
model_path = "./tmp/model.ckpt"
with tf.Session() as sess:
  sess.run(tf.global_variables_initializer())
  saver.restore(sess, model_path)
  print("Accuracy:", accuracy.eval({x: mnist.test.images, y_: mnist.test.labels}))
#+END_SRC
** Toy Program
#+BEGIN_CENTER
#+ATTR_HTML: :width 80%
[[file:images/toy_program.png]]
#+END_CENTER
* References
- [[https://github.com/PythonWorkshop/intro-to-tensorflow/blob/master/MathPrimer/Math%20primer%20for%20ML%20%26%20TensorFlow%20workshop.ipynb][Machine Learning Primer]]
- http://brohrer.github.io/how_convolutional_neural_networks_work.html
- https://www.tensorflow.org/get_started/mnist/beginners
- https://www.tensorflow.org/get_started/mnist/pros
- https://github.com/Hvass-Labs/TensorFlow-Tutorials

* Thank You
:PROPERTIES:
:SLIDE: thank-you-slide segue
:ASIDE: right
:ARTICLE: flexbox vleft auto-fadein
:END:

